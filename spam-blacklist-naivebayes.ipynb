{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import string\n",
    "import nltk\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'trec07p/data'\n",
    "LABELS_FILE = 'trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper methods for serializing our blacklist set\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class PythonObjectEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (list, dict, str, unicode, int, float, bool, type(None))):\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "        return {'_python_object': pickle.dumps(obj)}\n",
    "\n",
    "def as_python_object(dct):\n",
    "    if '_python_object' in dct:\n",
    "        return pickle.loads(str(dct['_python_object']))\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_to_string(parts):\n",
    "    ret = []\n",
    "    if type(parts) == str:\n",
    "        ret.append(parts)\n",
    "    elif type(parts) == list:\n",
    "        for part in parts:\n",
    "            ret += flatten_to_string(part)\n",
    "    elif parts.get_content_type == 'text/plain':\n",
    "        ret += parts.get_payload()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_email_text(path):\n",
    "    # Load a single email from an input file.\n",
    "    with open(path) as f:\n",
    "        msg = email.message_from_file(f)\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "\n",
    "    # Read the email subject.\n",
    "    subject = msg['Subject']\n",
    "    if subject:\n",
    "        subject = subject.decode(encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        subject = \"\"\n",
    "\n",
    "    # Read the email body.\n",
    "    body = ' '.join(m for m in flatten_to_string(msg.get_payload()) if type(m) == str)\n",
    "    if body:\n",
    "        body = body.decode(encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        body = \"\"\n",
    "\n",
    "    return subject + ' ' + body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    email_text = extract_email_text(path)\n",
    "    if not email_text:\n",
    "        return []\n",
    "\n",
    "    # Tokenize the message.\n",
    "    tokens = nltk.word_tokenize(email_text)\n",
    "\n",
    "    # Remove punctuation from tokens.\n",
    "    tokens = [i.strip(\"\".join(punctuations)) for i in tokens if i not in punctuations]\n",
    "\n",
    "    # Remove stopwords and stem tokens.\n",
    "    if len(tokens) > 2:\n",
    "        return [stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the labels.\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING SPAM_WORDS minus HAM_WORDS set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FILES 75419\n",
      "X_TRAIN: 52793\n",
      "X_TEST: 22626\n"
     ]
    }
   ],
   "source": [
    "# Split corpus into train and test sets.\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]\n",
    "\n",
    "print \"TOTAL FILES {}\".format(len(filelist))\n",
    "print \"X_TRAIN: {}\".format(len(X_train))\n",
    "print \"X_TEST: {}\".format(len(X_test))\n",
    "\n",
    "for filename in X_train:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        label = labels[filename]\n",
    "        stems = load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "        if label == 1:\n",
    "            ham_words.update(stems)\n",
    "        else:\n",
    "            spam_words.update(stems)\n",
    "\n",
    "blacklist = spam_words - ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 107944\n",
      "after: 100363\n"
     ]
    }
   ],
   "source": [
    "# clean non ascii blacklist words\n",
    "print('before: {}'.format(len(blacklist)))\n",
    "cleaned_blacklist = set() \n",
    "for w in iter(blacklist):\n",
    "    try:\n",
    "        w.decode('utf-8')\n",
    "        cleaned_blacklist.add(w)\n",
    "    except UnicodeError:\n",
    "        pass\n",
    "print('after: {}'.format(len(cleaned_blacklist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving blacklist to disk\n",
    "# with open('blacklist.json', 'w') as outfile:\n",
    "#     json.dump(cleaned_blacklist, outfile, cls=PythonObjectEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('blacklist.json', 'r') as infile:\n",
    "    bl = json.load(infile, object_hook=as_python_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 8251\n",
      "tn: 6660\n",
      "fp: 834\n",
      "fn: 5124\n"
     ]
    }
   ],
   "source": [
    "for filename in X_test:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        label = labels[filename]\n",
    "        stems = load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "        stems_set = set(stems)\n",
    "        if stems_set & bl:\n",
    "            if label == 1:\n",
    "                fp = fp + 1\n",
    "            else:\n",
    "                tp = tp + 1\n",
    "        else:\n",
    "            if label == 1:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "\n",
    "print \"tp: \" + str(tp)\n",
    "print \"tn: \" + str(tn)\n",
    "print \"fp: \" + str(fp)\n",
    "print \"fn: \" + str(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Machine Learning - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING DATA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Loading messages\n",
    "message_stems = [load(os.path.join(DATA_DIR, f)) for f in filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_stems = [' '.join(l) for l in message_stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(joined_stems)\n",
    "y = np.array([labels[f] for f in filelist])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAINING_SET_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.99      0.95      0.97     15068\n",
      "        Ham       0.91      0.98      0.94      7558\n",
      "\n",
      "avg / total       0.96      0.96      0.96     22626\n",
      "\n",
      "Accuracy 0.962\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES MACHINE LEARNING\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the classifier and make label predictions.\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print results.\n",
    "print(classification_report(y_test, y_pred, target_names=['Spam', 'Ham']))\n",
    "print(\"Accuracy {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
